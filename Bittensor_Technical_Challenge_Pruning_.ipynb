{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bittensor Technical Challenge - Pruning .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNyqE2/cG7GGUgjrEUznFVt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yasaman1997/16_Bit_Adder/blob/master/Bittensor_Technical_Challenge_Pruning_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfr_j2NivZm_",
        "outputId": "567b7d07-9d45-4108-ed4b-a0ec8382f2ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n"
          ]
        }
      ],
      "source": [
        "pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt \n",
        "from collections import OrderedDict\n",
        "from scipy.stats import rankdata\n",
        "from numpy import linalg\n",
        " \n",
        "import torch\n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as functional\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets"
      ],
      "metadata": {
        "id": "5ZUpe8bRv_ZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 784 # input size is 28x28, or 784\n",
        "num_epochs = 10\n",
        "batch_size = 64\n",
        "learning_rate = 0.01 \n",
        "transform = transforms.ToTensor() #convert images to tensor"
      ],
      "metadata": {
        "id": "qgYiRyyvwAgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Architecture\n",
        "# Biases are presumed FALSE since they have to be ignored\n",
        "\n",
        "\n",
        "class Magic_NN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(784, 1000, bias=False)\n",
        "        self.fc2 = nn.Linear(1000, 1000, bias=False)\n",
        "        self.fc3 = nn.Linear(1000, 500, bias=False)\n",
        "        self.fc4 = nn.Linear(500, 200, bias=False)\n",
        "        self.fc5 = nn.Linear(200, 10, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = functional.relu(self.fc1(x))\n",
        "        x = functional.relu(self.fc2(x))\n",
        "        x = functional.relu(self.fc3(x))\n",
        "        x = functional.relu(self.fc4(x))\n",
        "        x = functional.log_softmax(self.fc5(x), dim=1)\n",
        "        return x"
      ],
      "metadata": {
        "id": "gtb1RnRviOog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=Magic_NN()"
      ],
      "metadata": {
        "id": "pLjwGsVbicmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOYf5eJBBCia",
        "outputId": "9a617350-f036-41d0-ca5f-7262f73d79a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Magic_NN(\n",
              "  (fc1): Linear(in_features=784, out_features=1000, bias=False)\n",
              "  (fc2): Linear(in_features=1000, out_features=1000, bias=False)\n",
              "  (fc3): Linear(in_features=1000, out_features=500, bias=False)\n",
              "  (fc4): Linear(in_features=500, out_features=200, bias=False)\n",
              "  (fc5): Linear(in_features=200, out_features=10, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#download and process the train and test sets\n",
        "\n",
        "trainset = datasets.FashionMNIST('.', train=True, transform=transform,download=True)\n",
        "testset = datasets.FashionMNIST('.', train=False, transform=transform,download=True)"
      ],
      "metadata": {
        "id": "Uw4e9fzOiiMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loss function : CrossEntropy\n",
        "#Optimizer: Adam , Learning rate is 0.01\n",
        "\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "kpTF8FZHiw7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#list of indices for spliting data\n",
        "indices = list(range(len(trainset)))\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "#split to 80% for Train and 20% for Validation\n",
        "split = int(np.floor(0.2 * len(trainset)))\n",
        "train_sample = SubsetRandomSampler(indices[:split])\n",
        "valid_sample = SubsetRandomSampler(indices[split:])\n",
        "\n",
        "#prepare Data Loaders\n",
        "trainloader = torch.utils.data.DataLoader(trainset, sampler=train_sample, batch_size=batch_size)\n",
        "validloader = torch.utils.data.DataLoader(trainset, sampler=valid_sample, batch_size=batch_size)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "MfRFAAQV6JXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train The Model\n",
        "for epoch in range(num_epochs):\n",
        "  \n",
        "  #Train loss is: \"loss * batch size\", initialize it to 0\n",
        "  train_loss = 0 \n",
        "\n",
        "  model.train()\n",
        "\n",
        "  for image, label in trainloader:\n",
        "    # we must set the gradients to zero\n",
        "    optimizer.zero_grad()\n",
        "    #send the image to the network\n",
        "    output = model(image)\n",
        "    #calculate the loss\n",
        "    loss = loss_func(output, label)\n",
        "    #Backpropagation\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    #Calculate the train Loss\n",
        "    train_loss += loss.item() * image.size(0)\n",
        "\n",
        "\n",
        "  #evaluate the model   \n",
        "  model.eval()\n",
        "\n",
        "\n",
        "  #Validation Step\n",
        "  valid_loss = 0 #Validation :loss * batch size\n",
        "  min_valid_loss = np.Inf  #minimum value for validation loss\n",
        "\n",
        "  for image, label in validloader:\n",
        "    # forward pass the image through the network\n",
        "    output = model(image)\n",
        "    # Calculate the loss\n",
        "    loss = loss_func(output, label)\n",
        "    #Calculate the validation loss\n",
        "    valid_loss += loss.item() * image.size(0)\n",
        "\n",
        "\n",
        "  # Total train loss: AVG of train loss, also for validation\n",
        "  train_loss = train_loss/len(trainloader.sampler)\n",
        "  valid_loss = valid_loss/len(validloader.sampler)\n",
        "\n",
        "\n",
        "  print('Epoch: {} \\tTraining Loss: {:.4f} \\tValidation Loss: {:.4f}'.format(epoch+1, train_loss,valid_loss))\n",
        "    \n",
        "  # save model only if validation loss has decreased\n",
        "  if valid_loss <= min_valid_loss:\n",
        "      print('Validation loss decreased ({} : {}).  Save model!'.format(min_valid_loss,valid_loss))\n",
        "      torch.save(model.state_dict(), 'model.pt')\n",
        "      min_valid_loss = valid_loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKd9OrRj8fAY",
        "outputId": "3d17993c-aca6-452f-e4c7-48548d48b5ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 \tTraining Loss: 0.9575 \tValidation Loss: 0.6612\n",
            "Validation loss decreased (inf : 0.661185426513354).  Save model!\n",
            "Epoch: 2 \tTraining Loss: 0.6453 \tValidation Loss: 0.6523\n",
            "Validation loss decreased (inf : 0.6523093454440435).  Save model!\n",
            "Epoch: 3 \tTraining Loss: 0.6513 \tValidation Loss: 0.6256\n",
            "Validation loss decreased (inf : 0.6255972702900569).  Save model!\n",
            "Epoch: 4 \tTraining Loss: 0.5657 \tValidation Loss: 0.5719\n",
            "Validation loss decreased (inf : 0.5719396671454112).  Save model!\n",
            "Epoch: 5 \tTraining Loss: 0.5709 \tValidation Loss: 0.6333\n",
            "Validation loss decreased (inf : 0.6332964800198873).  Save model!\n",
            "Epoch: 6 \tTraining Loss: 0.5656 \tValidation Loss: 0.5697\n",
            "Validation loss decreased (inf : 0.5697133780121804).  Save model!\n",
            "Epoch: 7 \tTraining Loss: 0.6923 \tValidation Loss: 0.5926\n",
            "Validation loss decreased (inf : 0.5926126457850138).  Save model!\n",
            "Epoch: 8 \tTraining Loss: 0.5564 \tValidation Loss: 0.5713\n",
            "Validation loss decreased (inf : 0.5712916107177735).  Save model!\n",
            "Epoch: 9 \tTraining Loss: 0.5443 \tValidation Loss: 0.8452\n",
            "Validation loss decreased (inf : 0.8451660468379657).  Save model!\n",
            "Epoch: 10 \tTraining Loss: 0.5041 \tValidation Loss: 0.5525\n",
            "Validation loss decreased (inf : 0.5525068020621936).  Save model!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test \n",
        "\n",
        "\n",
        "def test(model, testloader, loss_func):\n",
        "    \n",
        "    test_loss = 0.0\n",
        "    #number of classes, and correct predictions\n",
        "    class_predicted_correct = list(0.0 for i in range(10))\n",
        "    total_classes = list(0.0 for i in range(10))\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for data, test_label in testloader:\n",
        "\n",
        "        # send image as input to the model\n",
        "        output = model(data)\n",
        "\n",
        "        # calculate the loss\n",
        "        loss = loss_func(output, test_label)\n",
        "      \n",
        "        # Test loss : loss * data size\n",
        "        test_loss += loss.item()*data.size(0)\n",
        "\n",
        "        # output probabilities must be converted to a class\n",
        "        _, pred = torch.max(output, 1)\n",
        "\n",
        "        # compare prediction with true label\n",
        "        correct = np.squeeze(pred.eq(test_label.data.view_as(pred)))\n",
        "\n",
        "        # calculate accuracy for each object class\n",
        "        for i in range(len(test_label)):\n",
        "            label = test_label.data[i]\n",
        "\n",
        "            class_predicted_correct[label] += correct[i].item()\n",
        "            total_classes[label] += 1\n",
        "\n",
        "    # AVG accuracy\n",
        "    overall_accuracy = 100. * np.sum(class_predicted_correct) / np.sum(total_classes)\n",
        "    return overall_accuracy"
      ],
      "metadata": {
        "id": "c0W6js4cC6Qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_wp = []\n",
        "accuracy_np = []\n",
        "\n",
        "#accuracy without pruning \n",
        "initial_accuracy = test(model, testloader, loss_func)\n",
        "accuracies_wp.append(initial_accuracy)\n",
        "accuracies_np.append(initial_accuracy)"
      ],
      "metadata": {
        "id": "qwe_jyDyMjA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pruning\n",
        "prune_percentage = [.0, .25, .50, .60, .70, .80, .90, .95, .97, .99]"
      ],
      "metadata": {
        "id": "IAXQLxMCxGyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Weight Pruning\n",
        "\n",
        "# Loop through each prune percent\n",
        "for k in prune_percentage[0:]:\n",
        "\n",
        "    # load the trained unprunned model\n",
        "    model = Magic_NN()\n",
        "    model.load_state_dict(torch.load('model.pt'))\n",
        "\n",
        "    # Get all the weights\n",
        "    weights = model.state_dict()\n",
        "\n",
        "    # keys to access model weights\n",
        "    layers = list(model.state_dict())\n",
        "    ranks = {} \n",
        "    pruned_weights = []\n",
        "\n",
        "    # For each layer of network(except output)\n",
        "    for l in layers[:-1]:\n",
        "        # weights for each layer + convert to numpy \n",
        "        data = weights[l]\n",
        "        w = np.array(data)\n",
        "\n",
        "        # Rank the weights element\n",
        "        ranks[l]=(rankdata(np.abs(w), method='dense') - 1).astype(int).reshape(w.shape)\n",
        "\n",
        "        # Get the threshold value based on the value of k(prune percentage) \n",
        "        lower_bound_rank = np.ceil(np.max(ranks[l]) * k).astype(int)\n",
        "\n",
        "        # Assign rank elements to 0 that are less than or equal to the threshold and 1 to those that are above.\n",
        "        ranks[l][ranks[l] <= lower_bound_rank] = 0\n",
        "        ranks[l][ranks[l] > lower_bound_rank] = 1\n",
        "\n",
        "        # Multiply weights array with ranks to zero out the lower ranked weights\n",
        "        w = w * ranks[l]\n",
        "\n",
        "        # Assign the updated weights as tensor to data and append to the pruned_weights list \n",
        "        data[...] = torch.from_numpy(w)\n",
        "        pruned_weights.append(data)\n",
        "\n",
        "    # Append the last layer weights as it is\n",
        "    pruned_weights.append(weights[layers[-1]])\n",
        "\n",
        "    # Update the model weights with all the updated weights \n",
        "    new_state_dict = OrderedDict()\n",
        "\n",
        "    #saving model state after prunning\n",
        "    for l, pw in zip(layers, pruned_weights):\n",
        "      new_state_dict[l] = pw\n",
        "    model.state_dict = new_state_dict\n",
        "\n",
        "    # append the test accuracy to accuracies_wp\n",
        "    accuracy_wp.append(test(model, testloader, loss_func))"
      ],
      "metadata": {
        "id": "kCmzDsuiWa-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_wp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bxNJMO711Jl",
        "outputId": "9370602a-b05b-468d-ae9b-8e196e866453"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[79.8, 79.64, 79.62, 79.66, 79.72, 79.7, 79.52, 79.17, 70.86, 29.43]"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Neuron Pruning\n",
        "\n",
        "\n",
        "for k in prune_percentage[0:]:\n",
        "   # load the trained unprunned model\n",
        "    model = Magic_NN()\n",
        "    model.load_state_dict(torch.load('model.pt'))\n",
        "\n",
        "    # Get all the weights\n",
        "    weights = model.state_dict()\n",
        "    layers = list(model.state_dict())\n",
        "    ranks = {}\n",
        "    pruned_weights = []\n",
        "\n",
        "    # For each layer of network(except output)\n",
        "    for l in layers[:-1]:\n",
        "      # weights for each layer + convert to numpy \n",
        "        data = weights[l]\n",
        "        w = np.array(data)\n",
        "\n",
        "        # taking norm for each neuron\n",
        "        norm = linalg.norm(w, axis=0)\n",
        "\n",
        "        # repeat the norm values to get the shape similar to that of layer weights\n",
        "        norm = np.tile(norm, (w.shape[0],1))\n",
        "\n",
        "        # Rank the weights element\n",
        "        ranks[l] = (rankdata(norm, method='dense') - 1).astype(int).reshape(norm.shape)\n",
        "\n",
        "        # Get the threshold value based on the value of k(prune percentage) \n",
        "        lower_bound_rank = np.ceil(np.max(ranks[l])*k).astype(int)\n",
        "\n",
        "        # Assign rank elements to 0 that are less than or equal to the threshold and 1 to those that are above.         \n",
        "        ranks[l][ranks[l] <= lower_bound_rank] = 0\n",
        "        ranks[l][ranks[l] > lower_bound_rank] = 1\n",
        "\n",
        "        # Multiply weights array with ranks to zero out the lower ranked weights\n",
        "        w = w * ranks[l]\n",
        "\n",
        "        # Assign the updated weights as tensor to data and append to the pruned_weights list \n",
        "        data[...] = torch.from_numpy(w)\n",
        "\n",
        "        # Append the last layer weights as it is\n",
        "        pruned_weights.append(data)\n",
        "\n",
        "\n",
        "    # Append the last layer weights as it is\n",
        "    pruned_weights.append(weights[layers[-1]])\n",
        "\n",
        "\n",
        "    # Update the model weights with all the updated weights \n",
        "    new_state_dict = OrderedDict()\n",
        "\n",
        "    #saving model state after prunning\n",
        "    for l, pw in zip(layers, pruned_weights):\n",
        "      new_state_dict[l] = pw\n",
        "\n",
        "    model.state_dict = new_state_dict\n",
        "    accuracy_np.append(test(model, testloader, loss_func))"
      ],
      "metadata": {
        "id": "cmY-p9oWWeen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracies_np"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3n0m8g8V2AsE",
        "outputId": "03ad5457-4de7-4b9a-d8c3-b787ae0f1fb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[82.43,\n",
              " 78.14,\n",
              " 69.94,\n",
              " 58.73,\n",
              " 53.09,\n",
              " 51.94,\n",
              " 42.9,\n",
              " 31.66,\n",
              " 22.73,\n",
              " 13.75,\n",
              " 82.41,\n",
              " 10.0,\n",
              " 10.0,\n",
              " 67.23,\n",
              " 13.75,\n",
              " 67.23,\n",
              " 13.75,\n",
              " 82.43,\n",
              " 79.8]"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "plot showing the percent sparsity (number of weights in the network that are zero) versus percent accuracy with two curves (one for weight pruning and one for unit pruning)"
      ],
      "metadata": {
        "id": "lpRHd02T0Zi4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the sparsity vs accuracy graph\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(prune_percentage, accuracy_wp, label='Weight Pruning')\n",
        "plt.plot(prune_percentage, accuracy_np, label='Neuron Pruning')\n",
        "plt.legend(loc='best')\n",
        "plt.xlabel('% sparsity')\n",
        "plt.ylabel('% accuracy')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "NvXs7O0lWl-r",
        "outputId": "80520e92-3c67-47dc-a603-6d64e516089e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEGCAYAAABiq/5QAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3wVZdbA8d/JTSWhhtA7KoggASIgKKKAgg07orsKi2Iv2Pd1V3FX14YNXUVcXLuioIIIrsqCjSIhIE1dFBEpKqD0lnLeP55JgyTchMwtyfl+Ptnc6WeIe2buM8+cR1QVY4wx1UdMuAMwxhgTWpb4jTGmmrHEb4wx1YwlfmOMqWYs8RtjTDUTG+4AglG/fn1t1apVuMMwxpiosnDhwk2qmrb//KhI/K1atSIzMzPcYRhjTFQRkR9Lmm9NPcYYU81Y4jfGmGrGEr8xxlQzlviNMaaa8TXxi8goEVkuIstE5HURSRSR1iIyX0S+E5GJIhLvZwzGGGOK8y3xi0hT4HogQ1U7AgHgQuBB4DFVPQz4HRjhVwzGGGMO5HdTTyyQJCKxQA1gA3ASMMlb/iJwls8xGGOMKcK3fvyquk5ExgBrgN3Ah8BCYIuq5nirrQWa+hXDgilPs2PLJn5J6cDGlCPQ2CRiBEQEERDEm4YYETcfCtYpum5MwTIBb7rotpA/L399ACm2r6LbCoX7KDYvRkrcVigSR7HfFIlbiscTU7ht0eMceC77xViCkma7vQSz3v7rlLBdsMcsJb5IpqqoguZ/Bm/azWe/6f3Xo4xl6haS533O0yLHU8jz1s0rmFd8Oq/IukW3z1OgYLrk7YvK/28h/88jBf9TyjIK/5YHzi/ca2nbBUQIxAixASE2RoiNiSn8HIghECPEBbx1Yty0KeRb4heRusBgoDWwBXgLGFiO7UcCIwFatGhRoRjiv3mXE3fPByBHY/ifNuervDYs0TYsyWvDt9qcnOh4h82UQzAXH7dexS5AJV3wSk3aJiKIUHiB8C4YgZiYgotDnHexKLYspviy/HVTEuKonRRHnRrud+2kOGoX+VwnKY5aSXHEBSK374yfWa8/8IOqbgQQkbeB3kAdEYn17vqbAetK2lhVxwPjATIyMir0f6HOt38I29aj6xYSWLeYI9dnceT6LIbumeWOEUggr2FHcht3IbdROjmNupBbty0qgRLulIrcPbnbIfK05Dukku6mSr3zytOCu7Wi6+YVJI/8fRUep9S7NpS8PArvyLRIjF5s6IExUmRfuSVkq2ATWEmD+uw/q6RdlbR/LWHNktc7+EqVeczS9uV9eSr4Jpk/jfftav/5IsXveA9Y5k275UX3UfQbq5tZ9Ftd/n73/wZY8nw44JtladsXOW5M0eMX+Tcp/PaiB85Di61c2nI9YLkW/3cv8t95Tp6Sk5dHbp6Snavk5uV5v5XsXDc/J0/JyXXr5eQVX5a/jVvurVPwWcnJddvszs4t+Jydm8eOPTls3Z3Nzn25JfzXUCg5PkCdGvHUSoqjdlIsdZLiOf6I+lzco2WZ24WCn4l/DdBTRGrgmnr6AZnALOA84A3gUmCKjzFArSZIrSZw5BluWhV+Xw3rs5B1WQTWLyKw5HVY+C+3PD4FGqdDk3Ro2hWadIW6rUq+jTTGVFvZuXls253Nlt3ZbM3/2VXk8+5stnjT23Zns3DN73zyv41ckNE87N8G/Gzjny8ik4AsIAdYhLuDfx94Q0Tu9eZN8CuGEolAvdbup+O5bl5eLmxaCeuzYF0WrF8EXz4HuXvd8qS60KSLuwjkXwxqNQ5p2MaYyBIXiCE1JYHUlISg1p+xdANXvZrFkrVb6dayrs/Rlc3XBm5VvRu4e7/Zq4Dufh633GIC0KC9+0m/yM3L2Qe/rnAXg/WLYN0i+PwxUO/rXUqjwotAky7uc4164TsHY0xE697a5Yd5qzZX7cQf1WLjXXNPk/TCeft2wc9L3YUg/9vBt9MLl9dp6V0MvG8HTdIhoWboYzfGRJzUlASOaJjC/B9+45oTwxuLJf7yiK8BLXq4n3x7tsKGr7wmoixYtxCWv+MtFKh/ROE3giZdoVFHiEsKS/jGmPDq2SaVSQvXkp2bF9Z2fkv8hyqxNrTu437y7dzkfStY5C4Iq2bBkjfcsphYaHBk8ecFDY6EQFx44jfGhEzPNqm8NPdHlq7bStcW4WvuscTvh+T6cPgA9wOuJ9G29cWbiFZMgawX3fLYRGjUqfjzgtTDISZy+wEbY8ovv51//qrfLPFXeSJQu6n7OfJ0N08Vfv+hsBfRuixY9Ap8+axbHl+z8BlD/reDOi2tW6kxUax+SgKHN0hh3qrNXNW3bdjisMQfLiJQr4376XSem5eXC5v+V/i8YP0imP8s5O5zy5PqFX9e0LQr1GwUvnMwxpRbzzapvJ21lpzcPGLD1M5viT+SxARce3+DI6HLxW5ezj74dXnht4L1i+CzRwu7ldZs7F0EuhQ2FVm3UmMiVo829Xh53o8sW7+N9OZ1whKDJf5IFxvvdQ/tAhl/cvMKupVmFX47+Pb9wm3qtir+vKBxZ+tWakyE6NE6FXD9+S3xm+CV1q10/eLCi8HaTFj+trdQIK1d8bePG3aEuMSwhG9MdZZWM4HDvHb+K08ITzu/Jf6qIrE2tDnB/eTbsbGwJ9H6RfDdTPjqdbcsJhYadCj+vCCtvXUrNSYEerSux5TF68PWzm+JvypLSYMjTnY/4HUrXVfkeUGWe9ls4QtueWwiNDoa2p8KGSMgsVbYQjemKuvZJpVX569h+fptdA5Dc48l/upEBGo3cz9Fq5X+tqrwYrD2S/h4tKtL1P0K6HmVPSw2ppL1aFNYtyccid/eEKruRCC1retSOvAfcNnHMHK2exP504fgsY7wnzth24ZwR2pMldGgZiJt0pKZ/8NvYTm+JX5zoCZdYMgrcPV898LZvGfgiaNh2ij4/cdwR2dMldCzTSoLfviN3LzQD9Vmid+UrkF7OGc8XLfQlate9AqM7QLvXAkb/xfu6IyJaj1a12P73hxWrN8W8mNb4jcHV681nPEE3PAV9LgClr8L/+wOb17iKpMaY8qtZ5vC/vyhZonfBK9WExh4P4xaBsffBN/Pgmf7wKvnw5r54Y7OmKjSsFYibeonW+I3USK5PvS7y10ATvqrG4Pg+ZPhhdPdxSDY0dmNqeZ6tKnHl6tD385vid9UXGJt6HML3LgUTrkfNn8HL58F/+oH37wPeXnhjtCYiNazTSrb9+Tw9YbQtvNb4jeHLj4Zjr3aPQM4/XE3EM0bF8G43rB0kqs6aow5QNG6PaFkid9UntgEyBgO12XB2eNB82DyCHgqA7JecpVGjTEFGtVOpFVqDeatCm1/ft8Sv4i0E5HFRX62iciNIlJPRD4SkZXe7/AON28qXyAWOg+Bq+bCBS+7yqBTr3NdQec/C9m7wx2hMRGjZ5tUvvxhMxrCZ2O+JX5V/VZV01U1HegG7ALeAe4AZqrq4cBMb9pURTEx0OFMGPkJXDwZ6jSHGbfB451cSYg9oe+/bEykaZmazLY9OezODl2TaKiaevoB36vqj8BgwBtslheBs0IUgwkXETi8P/zpAxg+wxWC+3g0PN4RZv0DdoXntXVjIkFKQgCAnXurXuK/EPDqAdNQVfMLv/wMNCxpAxEZKSKZIpK5cePGUMRoQqFlL/jj23D5LGh1PHzyYGE9oO0/hzs6Y0IuOcHVyty5Nydkx/Q98YtIPHAm8Nb+y9Q1apXYsKWq41U1Q1Uz0tLSfI7ShFzTrnDhq3D1PGh/Gsx7Gh4/GqbdZPWATLVSI94l/h1VKfEDg4AsVf3Fm/5FRBoDeL9/DUEMJlI1OBLOfc7VA+p8oev982RXeOcq2LQy3NEZ47uUqnjHDwylsJkHYCpwqff5UmBKCGIwka5eGzhzrHsX4JjL3QAxTx0Db14KG5aEOzpjfJPstfHv2ldF2vhFJBkYALxdZPYDwAARWQn096aNcWo3hUEPuLeBjxsF3/8Xnj0eXr0Afvoy3NEZU+ny7/irTFOPqu5U1VRV3Vpk3mZV7aeqh6tqf1W1Lh3mQClp0P9udwE46S+wdgFMGODqAa2abfWATJVRo4o29RhTcUl1oM+triDcKf9w9YBeGuzVA5puFwAT9VKq6MNdYw5dfDIce41XD+gxrx7QUHjG6gGZ6JZchfvxG1M5YhMg409ePaBnIS/Hqwd0DGS9bPWATNSJDcSQEBvDrn12x29M2QKxrvvn1fPggpfcN4Kp13r1gMZbPSATVZITYq2px5igxcRAh8Fwxadw8SSo3Qxm3OpeBvv8casHZKJCckLAHu4aU24icPgAGPEfGDYdGnWEj++2ekAmKiTHx7LD2viNOQStesMf34HL/1u8HtCHf4Htvxx8e2NCLCUh1tr4jakUTbu5ekBXzYX2p8Lcf7qS0O/fDFvWhDs6YwokJ8RaU48xlaphBzj3X3BtphsgZuGL7iHwu1dbPSATEZITAvZw1xhfpLaFM5+EGxbDMZfBsrddN9C3hsHPS8MdnanGkuNjrR+/Mb6q3QwGPVhYD2jlxzDuOKsHZMImOSGWndbGb0wI5NcDGrUMTrR6QCZ8Urw2/lCNu2uJ35ikOnDCre4bwMn3uXb/lwbDv/rDtzPsAmB8VyMhQJ7Cnuy8kBzPEr8x+RJSoNe1rh7QaY/Czl/h9QtdM9CyyVYPyPgm1KWZLfEbs7+4RDhmhKsHdNY4yN0Hk/4E/+wOi16B3OxwR2iqmGSvQmeo+vJb4jemNIE4SB8KV8+H81+EuBow5RrXFfTL56wekKk0yXbHb0yEiYmBo84qrAdUqylMv6WwHtDe7eGO0ES5UJdmtsRvTLDy6wH96QMY9j40PMrVA3qsI3z+GGTvCXeEJkolh3gULkv8xpSXCLQ6Di5519UDat4DPh7tXgZbOsl6AZlyy3+4G6q+/Jb4jTkUTbvBxW/Cpe9BUm03KMyEAfYimCkXu+M3Jhq17gMjP4HBT8OWn1zyf2s4/L463JGZKFA47m4VaOMXkToiMklEvhGRr0XkWBGpJyIfichK73ddP2MwJmRiAtDlYrhuIZxwh3v566lj4KO7YM/WcEdnIliNgoe7VeOO/wngA1VtD3QGvgbuAGaq6uHATG/amKojIQVO/DNcnwUdz4MvxhZ2Ac0NXT0WEz3iAjHEx8ZEfxu/iNQG+gATAFR1n6puAQYDL3qrvQic5VcMxoRVrSZw9jMwcjY06OC6gD7TC/73H3sAbA6QEsKa/H7e8bcGNgL/FpFFIvIvEUkGGqrqBm+dn4GGJW0sIiNFJFNEMjdu3OhjmMb4rEm6e/h74WuQlwOvXQAvnwU/Lwt3ZCaCuHF3o7+NPxboCjyjql2AnezXrKOuFF2Jtz6qOl5VM1Q1Iy0tzccwjQkBEWh/Glw9DwY+COsXuxpAU66F7T+HOzoTAdy4u9F/x78WWKuq873pSbgLwS8i0hjA+/2rjzEYE1li46HnlXD9Iuh5NXz1BoztCp88DPt2hTs6E0ahHH7Rt8Svqj8DP4lIO29WP2AFMBW41Jt3KTDFrxiMiVg16sHAf8A18+Gwk2DWvfBUhrsQ5IWmNK+JLG4wluhv6gG4DnhVRJYA6cA/gAeAASKyEujvTRtTPaW2hSGvwLDpkJwG71wBz50Iq78Id2QmxFISAiG744/1c+equhjIKGFRPz+Pa0zUadUbLp8FS9+CmffAC6dC+9NhwN/cxcFUeTXiq0BTjzGmnGJioPMQuDbTDQX5/Sz4Zw/44P9g9+/hjs74LCWhajzcNcZURHwNNxTk9VluPIB5T7sXwOY9Azn7wh2d8UlyQoBd+3JDMu6uJX5jIlXNRnDmk3DlZ9C4M3xwBzzdE755314Aq4KSE2LJzVP25vj/cN8SvzGRrlEn+OO7cNFbrh7QGxfBC6e7dwFMlRHKcXct8RsTDUTgiJPhqjlw6hjY+DWM7wvvXAV7toU7OlMJasSHrjSzJX5jokkgDrpf7l4A6309LH0TXj3Phn+sAlJCOPyiJX5jolFibdfV87znYW0mvHo+7N0R7qjMIUgO4ShcB038ItLJ9yiMMRXTYTCcN8GN+PXaBbBvZ7gjMhWUHGFt/E+LyJcicrVXatkYE0mOOhvOfQ7WzIXXhljNnyiVHElt/Kp6PHAx0BxYKCKvicgA3yMzxgSv47lw9nj48Qt43ZJ/NEr22vh3RUobv6quBP4C3A6cAIz1hlM8x8/gjDHlcPT5cNY4+OEzeGMoZO8Od0SmHCKqO6eIHC0ij+GGTTwJOENVj/Q+P+ZzfMaY8ug8BM56BlZ94vr7Z+8Jd0QmSJHWnfNJIAvorKrXqGoWgKqux30LMMZEkvShMPgpV+tn4sWW/KNEfGwM8YEYdoSgV08w1TlPA3arai6AiMQAiaq6S1Vf9jU6Y0zFdPkDaB5MvQ7e/KMr/RybEO6ozEEkJwQipo3/YyCpyHQNb54xJpJ1vQROfxxWfghvXgI5e8MdkTmIUI3CFUziT1TVgjdDvM81/AvJGFNpMobDaY/C/z6At4ZZdc8IF6rSzMEk/p0i0jV/QkS6AdZdwJhoccwIV9/n2+kwaTjkZoc7IlMKN/xiZLTx3wi8JSLrAQEaAUN8jcoYU7m6X+7a/Gfc5pL/ef92dX9MRKkRH2DbnghI/Kq6QETaA/mDpn+rqnbLYEy06XGFS/4f3AGTL4NzJ0DA19FXTTmlJMTy81b/e2EF+1dvB3QAEoGuIoKqvuRfWMYYX/S8CvJy4cM7QWLgnOcs+UeQUD3cPehfXETuBvriEv90YBDwOWCJ35ho1Ota0Fz46C6X/M9+1pJ/hAjVw91g/trnAZ2BRao6XEQaAq8Es3MRWQ1sB3KBHFXNEJF6wESgFbAauEBVbSRpY0Kp9w2u2efj0V7yH+dG9zJhVSM+wE5v3F0R8e04wfTq2a2qeUCOiNQCfsUVbAvWiaqarqoZ3vQdwExVPRyY6U0bY0LtuFFw0l/dYC7vXu2agExYhWrc3WDu+DNFpA7wHLAQ2AHMPYRjDsY1HQG8CMzGFX8zxoRan1vcnf+s+9wd/5lPQYyNzxQu+YXadu7NITHOv29gZSZ+cd817lfVLcA4EfkAqKWqS4LcvwIfiogCz6rqeKChqm7wlv8MNCzl2COBkQAtWrQI8nDGmHI74TaX/Gff78b2PeNJS/5hUjAK195cUlP8O06ZiV9VVUSmA5286dXl3P9xqrpORBoAH4nINyXsX0s59nhgPEBGRkaJ6xhjKknfO1xTz6cPuTb/05+w5B8GyfHuLt/vB7zBNPVkicgxqrqgvDtX1XXe719F5B2gO/CLiDRW1Q0i0hj3zMAYE24n/p/r7fPZIyABV+rBkn9I5d/x7/L57d1g/qo9gLki8r2ILBGRpSJy0KYeEUkWkZr5n4GTgWXAVOBSb7VLgSkVC90YU6lE3MPe3jfCwn/D9FtA7ct2KIVq3N1g7vhPqeC+GwLveF2SYoHXVPUDEVkAvCkiI4AfgQsquH9jTGUTgf6jXZv/nLHuge+gh9x847uUIm38fgom8Vfokq+qq3D9//efvxnoV5F9GmNCQAQG/M0l/7lPuTb/gQ9Y8g+BGl4bv99v7waT+N/HJX/BlWxoDXwLHOVjXMaYcBKBk+91D3znP+Pa/E+5z5K/zwru+H1u4w+mSFunotNeiearfYvIGBMZRGDg/e7Of94/Cy8Glvx9k5wQmnF3y12gQ1WzRKSHH8EYYyKMCAx6sLDZJyYA/e+x5O+TgnF3w93GLyI3FZmMAboC632LyBgTWUTg1IddV88vnnDNPv3usuTvk+SEQETc8dcs8jkH1+Y/2Z9wjDERSQROfcS1+X/+qLvzP/FOS/4+qBHv/yhcwbTx3+NrBMaY6BAT4wZv1zz49GE3z5J/pUsJQU3+g77AJSIfeUXa8qfrish/fI3KGBOZYmLgjLHQ5Q8u+b9xMey2quqVyTX1+NvGH8ybu2lekTYAvNr5DfwLyRgT0WJiXBXPU/4BK/8Dz/aBdQvDHVWVkRyCwViCSfy5IlJQHlNEWlLBl7qMMVWECBx7DQz/wJV1mHAKzBtnJR4qQXJ8bETU6rkT+FxEXhaRV4BPgT/7GpUxJjo0Pwau+BQO6w8f3A5vXgJ7toY7qqjmxt0Nc1OPqn6A68I5EXgD6Kaq1sZvjHFq1IOhr8OAv8M377umn/WLwx1V1EpJCIS/qUdEzgayVXWaqk7DDcF4lq9RGWOiiwj0vh6Gz4DcbJgwAL58zpp+KiDZ69WjPv7bBdPUc7eqFnx38x703u1bRMaY6NWiB1zxGbQ+wZV1njQc9mwLd1RRJTkhlhyfx90NJvGXtE65Sz0YY6qJ5FS46E3odzesmArjT4ANwY7WavJH4dq1z792/mASf6aIPCoibb2fR3GDrhtjTMliYuD4m2DYNMjeDf/qD5nPW9NPEEJRqC2YxH8dsA/3cHcisBe4xreIjDFVR8tecOXn0Oo4mDYKJl8Ge7eHO6qIlhKCUbiCKdmwE7jDtwiMMVVbcn24eBJ8/gjM+gdsWAznvwiNOoY7sogUEXf8IpImIg+LyHQR+W/+j28RGWOqnpgY6HMrXPoe7N0B/+oHC1+0pp8SJCd4o3CFuY3/VeAb3Mhb9wCrgQW+RWSMqbpaHQdXfgYtesJ718M7V7gLgSkQEXf8QKqqTsD15f9EVf8EnORbRMaYqi2lAfzhbej7f7DkTXjuRPhlRbijihjJ8f638QeT+LO93xtE5DQR6QLU8y0iY0zVFxOAvrfDJVNg9xZ47iRY9Gq4oyqZKvy8DL4YC69dCKtm+3q4lBDc8QfTH/9eEakN3Aw8CdQCRgV7ABEJAJnAOlU9XURa40o/pOK6hf5RVfeVO3JjTPRrc4Lr9TN5BEy5GlZ/DqeNgfjk8Ma1Y6NL8N/PhO//Czt+cfNjk+CXZXDNlxBfw5dD10jwvx9/ML16pnkftwInVuAYNwBf4y4YAA8Cj6nqGyIyDhgBPFOB/RpjqoKaDd2d/ycPwicPwfos1+unQfvQxZCzD36a75L89zNhw1duflJdaHMiHNYP2p4Ev6+Gfw9yo5Cd9BdfQkmIDRAXkPB25zwUItIMOA24D7hJRAT3fOAib5UXgdFY4jemeosJwIn/5x76Tr7ctfuf/hh0vtCf46nCb6vgO++OfvVnsG8HxMRCs+4uqbc9CRqnu9jy1WoCnS5wYw93HgqpbX0JL9nnUbj8Lr3wOHAbheP2pgJbVDX/jNYCTUvaUERGAiMBWrRoUdIqxpiqpu1JhU0/71zhEvKghyunWWXPVvjhUy/Zz4Qta9z8uq3h6CHurr7V8ZBYq+z9nPx3+HY6fPBnuPjNQ4+rBMnx/g7G4lviF5HTgV9VdaGI9C3v9qo6HhgPkJGRYZ19jakuajWGS6bC7PvhszGwzmv6STuifPvJy4X1i9wd/XczYe0C0FyIrwmt+0DvG9yFpl6b8u23ZiPoewd8+Bf49gNoN7B82wchOSHALh9r8ged+EWkJ65ZJhF4XFXfPcgmvYEzReRUb5tawBNAHRGJ9e76mwHrKhK4MaYKC8RCv79Cy2Ph7ZEwvi+c8QQcfX7Z221dW5joV82GPVsAgSbpcNwod1ff7BgIxB1afD2uhKyX3eAzbfpCXOKh7W8/yQmx7PRxFK5SE7+INFLVn4vMugk4GxBgPlBm4lfVP+ON1OXd8d+iqheLyFvAebiePZcCUw7lBIwxVdhh/V2Z58kj4O3L4MfPYeADEJfklu/bBT9+UZjsN33r5tdsDO1Ph7YnuoezyamVG1cgDk59CF4aDHPGwgm3VeruU3wed7esO/5xIpIFPKSqe4AtuISdBxxKge3bgTdE5F5gETDhEPZljKnqajeFS6fBrHvh88dg7ULoeA788An8OBdy90JsoisI1/US13zT4Eg3OIyf2vSFDmfBZ4+4ZwR1W1barpPjY/ll255K29/+Sk38qnqWiJwBTBORl4Abcb1xagDlGoFLVWcDs73Pq4DuFYzXGFMdBWKh/2ho0QveGQkz74EGHaD75S7Rt+xV+C0glE65D1Z+CB/eCUNeqbTd+j3ubplt/Kr6nohMB64G3gHuU9VPfYvGGGPKcsTJcMMSyN7lHrKGW+1m0OcWmPk319R0WL9K2W1yQsDXNv5SSzaIyJkiMgv4AFgGDAEGi8gbIuJP51VjjDmYxFqRkfTzHXut6xk043b3Ilgl8Lsff1m1eu4FBgEXAA+q6hZVvRn4K+6FLGOMMbEJMOgh2LwS5j1dKbtMSYglO1fZm+NPc09ZiX8rcA5wLvBr/kxVXamqPr1OZ4wxUejwAdDuNFdyYtv6Q95d/ri7frXzl5X4z8a9aRtLYYkFY4wxJRn4D8jLcS92HaIaPlfoLDXxq+omVX1SVcep6qF03zTGmKqvbiv3ktiyyfDDZ4e0q4LSzD494A2mHr8xxphgHHcj1GkBM26D3OyDr18Kv0fhssRvjDGVJS7JvVn86wr48rkK7ybFq8m/Iwxt/MYYY8qr3amu1MTs+2H7LxXaRQ1v+MVddsdvjDFRQAQGPgjZu+Hj0RXaRX4bv1/1eizxG2NMZat/GPS6Dr56DdbML/fm1sZvjDHRqM8tUKspTL/ZjQ1QDsleG/9On8bdtcRvjDF+iE+Gk++Fn5fCwn+Xa9P8cXftjt8YY6LNUWe70b5m/h12bi7XpjXi/avXY4nfGGP8IuLGDN63w5WSLgc3GIs19RhjTPRp0N4bqvElWLcw6M2SEwJ2x2+MMVHrhNshpQF88OegN/Fz3F1L/MYY47fEWtDnVvhpPvy0IKhNkq2N3xhjolznoZBQC+aPC2p119RjbfzGGBO9ElKgyx9hxbuwbcNBV09OiI2+N3dFJFFEvhSRr0RkuYjc481vLSLzReQ7EZkoIvF+xWCMMRGl++XuZa7M5w+6akpCLLuisI1/L3CSqnYG0oGBItITeBB4TFUPA34HRvgYgzHGRI56raHdIJf4c/aWuarrxx9lTT3q7PAm47wfBU4CJnnzXwTO8isGY4yJOD2ugF2b3IAtZUhJCLAvN499OXmVHoKvbfwiEhCRxbgxez8Cvge2qGr+95e1QNNSth0pIpkikrlx4wY++/8AABRpSURBVEY/wzTGmNBpfQKktYd5z4Bqqav5WajN18Svqrmqmg40A7oD7cux7XhVzVDVjLS0NN9iNMaYkBJxd/0/L3HdO0uR7OPwiyHp1aOqW4BZwLFAHRGJ9RY1A9aFIgZjjIkYRw+BxNpldu2skxRHg5oJ0dXUIyJpIlLH+5wEDAC+xl0AzvNWuxSY4lcMxhgTkeKToeulsGIqbC353vfkoxrx5Z39aZOWUumH9/OOvzEwS0SWAAuAj1R1GnA7cJOIfAekAhN8jMEYYyLTMZcBCpmhT4GxB1+lYlR1CdClhPmrcO39xhhTfdVt6cbnzfy3K+cQlxSyQ9ubu8YYEy49roTdv8HSSQdftxJZ4jfGmHBpdRw0OArmPxvSw1riN8aYcBGBzkPgl6Ww67eQHdYSvzHGhFP9du735u9DdkhL/MYYE06ph7nfm78L2SEt8RtjTDjVbQkSsMRvjDHVRiDOJf/frKnHGGOqj9TD7I7fGGOqlXptYfOqMqt1ViZL/MYYE26pbSF7J2z/OSSHs8RvjDHhFuKePZb4jTEm3FLbut+W+I0xppqo1QwCCSHr2WOJ3xhjwi0mxt31h+jtXUv8xhgTCeq1saYeY4ypVlIPg99+gLxc3w/l20AsfsvOzmbt2rXs2bMn3KGYCkpMTKRZs2bExcWFOxRjwi+1LeRlw5Y1UK+1r4eK2sS/du1aatasSatWrRCRcIdjyklV2bx5M2vXrqV1a3//IzcmKuR36fzte98Tf9Q29ezZs4fU1FRL+lFKREhNTbVvbMbkK+jL7/8D3qhN/IAl/Shnfz9jikhOg/iaIXnAG9WJ3xhjqgyRkHXp9C3xi0hzEZklIitEZLmI3ODNryciH4nISu93Xb9i8NOoUaN4/PHHC6ZPOeUULrvssoLpm2++mUcffbTU7e+66y4+/vjjMo8xevRoxowZc8D8LVu28PTTT5e6XSAQID09nY4dO3L++eeza9euMo8TrMzMTK6//vpK2ZcxpgQhqtLp5x1/DnCzqnYAegLXiEgH4A5gpqoeDsz0pqNO7969mTNnDgB5eXls2rSJ5cuXFyyfM2cOvXr1KnX7v/3tb/Tv379Cxz5Y4k9KSmLx4sUsW7aM+Ph4xo0bV2x5Tk5OhY6bkZHB2LFjK7StMSYIqW1h60+Qs9fXw/jWq0dVNwAbvM/bReRroCkwGOjrrfYiMBu4/VCOdc97y1mxftuh7OIAHZrU4u4zjip1ea9evRg1ahQAy5cvp2PHjmzYsIHff/+dGjVq8PXXX9O1a1cWLlzITTfdxI4dO6hfvz4vvPACjRs3ZtiwYZx++umcd955TJ8+nZtuuonk5GR69+7NqlWrmDZtGgArVqygb9++rFmzhhtvvJHrr7+eO+64g++//5709HQGDBjAww8/XGqcxx9/PEuWLGH27Nn89a9/pW7dunzzzTd8+OGHnH766SxbtgyAMWPGsGPHDkaPHk3fvn3p0aMHs2bNYsuWLUyYMIHjjz+e2bNnM2bMGKZNm8bo0aNZs2YNq1atKhYbwN///ndeeeUV0tLSaN68Od26deOWW26prD+NMVVX6mGgefD7akhr59thQtKdU0RaAV2A+UBD76IA8DPQsJRtRgIjAVq0aOF/kOXUpEkTYmNjWbNmDXPmzOHYY49l3bp1zJ07l9q1a9OpUydEhOuuu44pU6aQlpbGxIkTufPOO3n++ecL9rNnzx6uuOIKPv30U1q3bs3QoUOLHeebb75h1qxZbN++nXbt2nHVVVfxwAMPsGzZMhYvXlxmjDk5OcyYMYOBAwcCkJWVxbJly2jdujWrV68+6LZffvkl06dP55577imxWaqk2BYvXszkyZP56quvyM7OpmvXrnTr1i3If1VjqrmixdqiOfGLSAowGbhRVbcV7cmhqioiJY48oKrjgfEAGRkZZY5OUNaduZ969erFnDlzmDNnDjfddBPr1q1jzpw51K5dm969e/Ptt9+ybNkyBgwYAEBubi6NGzcuto9vvvmGNm3aFPRlHzp0KOPHjy9Yftppp5GQkEBCQgINGjTgl19+OWhcu3fvJj09HXB3/CNGjGDOnDl079496D7z55xzDgDdunUr9SJRUmxffPEFgwcPJjExkcTERM4444ygjmeMwQ3IAr638/ua+EUkDpf0X1XVt73Zv4hIY1XdICKNgV/9jMFP+e38S5cupWPHjjRv3pxHHnmEWrVqMXz4cFSVo446irlz51b4GAkJCQWfA4FAUO3z+W38+0tOTi74HBsbS15eXsH0/v3p849b1jErEpsxpgxJdaBGfd979vjZq0eACcDXqlq0e8tU4FLv86XAFL9i8FuvXr2YNm0a9erVIxAIUK9ePbZs2cLcuXPp1asX7dq1Y+PGjQWJPzs7u9gDYIB27dqxatWqgrvqiRMnHvS4NWvWZPv27YcUe8OGDfn111/ZvHkze/fuLXimcKh69+7Ne++9x549e9ixY0el7deYaiMEXTr97NXTG/gjcJKILPZ+TgUeAAaIyEqgvzcdlTp16sSmTZvo2bNnsXm1a9emfv36xMfHM2nSJG6//XY6d+5Menp6QU+gfElJSTz99NMMHDiQbt26UbNmTWrXrl3mcVNTU+nduzcdO3bk1ltvrVDscXFx3HXXXXTv3p0BAwbQvn37Cu1nf8cccwxnnnkmRx99NIMGDSr49zDGBCn1MN/r8ouGaHDfQ5GRkaGZmZnF5n399dcceeSRYYqocu3YsYOUlBRUlWuuuYbDDz+8oMdQNMo/n127dtGnTx/Gjx9P165dS1y3Kv0djakUnz0CM/8Gf14HCSmHtCsRWaiqGfvPtzd3I8Bzzz1Heno6Rx11FFu3buWKK64Id0iHZOTIkaSnp9O1a1fOPffcUpO+MaYE+Q94fbzrj9rqnFXJqFGjovoOf3+vvfZauEMwJnoVLdbWuLMvh7A7fmOMiST12rjfPj7gtcRvjDGRJL4G1Grqa19+S/zGGBNpUtv62sZvid8YYyKNz1U6LfEfAhHh5ptvLpgeM2YMo0ePDl9ARQwbNozWrVsX9K45lLeH93fqqaeyZcuWStufMWY/9drC7t9h12++7N4S/yFISEjg7bffZtOmTZW6X1UtVk6hoh5++GEWL17MAw88UGIX0dzc3Artd/r06dSpU+dQwzPGlMbnYRirRnfOGXfAz0srd5+NOsGgsl8qjo2NZeTIkTz22GPcd999xZZt3LiRK6+8kjVr1gDw+OOP07t3b0aPHk1KSkpBmeKOHTsWlDU45ZRT6NGjBwsXLmT69Ok89dRTzJgxAxHhL3/5C0OGDGH27NmMHj2a+vXrs2zZMrp168Yrr7xS5jCGffr04bvv3NfGVq1aMWTIED766CNuu+02xo0bx5gxY8jIyGDTpk1kZGSwevVqXnjhBaZOncquXbv4/vvvOfvss3nooYcK9pGZmcmOHTsYNGgQxx13HHPmzKFp06ZMmTKFpKQkFixYwIgRI4iJiWHAgAHMmDGjoAS0MeYgilbpbH5Mpe/e7vgP0TXXXMOrr77K1q1bi82/4YYbGDVqFAsWLGDy5MnFRucqzcqVK7n66qtZvnw5mZmZLF68mK+++oqPP/6YW2+9lQ0bXDXrRYsW8fjjj7NixQpWrVrFF198UeZ+33vvPTp16lQwnZqaSlZWFhdeeGGZ2y1evJiJEyeydOlSJk6cyE8//VRizNdccw3Lly+nTp06TJ48GYDhw4fz7LPPsnjxYgKBwEHP3RhTRJ2WIAHf2vmrxh3/Qe7M/VSrVi0uueQSxo4dS1JSUsH8jz/+mBUrVhRMb9u2jR07dpS5r5YtWxbU/fn8888ZOnQogUCAhg0bcsIJJ7BgwQJq1apF9+7dadasGQDp6emsXr2a44477oD93Xrrrdx7772kpaUxYcKEgvlDhgwJ6tz69etXUGenQ4cO/PjjjzRv3rzYOvnPEaCwhPOWLVvYvn07xx57LAAXXXSRFWszpjxi46FuS9969lSNxB9mN954I127dmX48OEF8/Ly8pg3bx6JiYnF1i2rHHLRssllCbYc8sMPP8x55513wPzSyjOXVpq5rOPsv87u3buDOgdjzEHUa+vbHb819VSCevXqccEFFxS7qz755JN58sknC6bz6+O3atWKrKwswI2I9cMPP5S4z+OPP56JEyeSm5vLxo0b+fTTT+nevXulx96qVSsWLlwIwKRJkypln3Xq1KFmzZrMnz8fgDfeeKNS9mtMtZJ6GGxeBT4U0rTEX0luvvnmYr17xo4dS2ZmJkcffTQdOnQoGPD83HPP5bfffuOoo47iqaee4ogjjihxf2effTZHH300nTt35qSTTuKhhx6iUaNGlR73LbfcwjPPPEOXLl0qtXfShAkTuPzyy0lPT2fnzp1WmtmY8kptC9k7YfvPlb5rK8tsfJFfmhnggQceYMOGDTzxxBMHrGd/R2NKsTYT5j4F/e9x7f0VUFpZZmvjN754//33uf/++8nJyaFly5a88MIL4Q7JmOjSLAPOf8GXXVviN74YMmRI0L2HjDGhFdVt/NHQTGVKZ38/Y8IjahN/YmIimzdvtuQRpVSVzZs3H9Dd1Rjjv6ht6mnWrBlr165l48aN4Q7FVFBiYmLBi2jGmNCJ2sQfFxdH69atwx2GMcZEnaht6jHGGFMxlviNMaaascRvjDHVTFS8uSsiG4EfK7h5faByR0qJDtXxvKvjOUP1PG875+C0VNW0/WdGReI/FCKSWdIry1VddTzv6njOUD3P28750FhTjzHGVDOW+I0xppqpDol/fLgDCJPqeN7V8Zyhep63nfMhqPJt/MYYY4qrDnf8xhhjirDEb4wx1UyVSfwiMlBEvhWR70TkjhKWJ4jIRG/5fBFpFfooK1cQ53yTiKwQkSUiMlNEKjaMT4Q52HkXWe9cEVERifpuf8Gcs4hc4P29l4vIa6GO0Q9B/DfeQkRmicgi77/zU8MRZ2USkedF5FcRWVbKchGRsd6/yRIR6Vrug6hq1P8AAeB7oA0QD3wFdNhvnauBcd7nC4GJ4Y47BOd8IlDD+3xVtJ9zsOftrVcT+BSYB2SEO+4Q/K0PBxYBdb3pBuGOO0TnPR64yvvcAVgd7rgr4bz7AF2BZaUsPxWYAQjQE5hf3mNUlTv+7sB3qrpKVfcBbwCD91tnMPCi93kS0E9EJIQxVraDnrOqzlLVXd7kPKAq1EAO5m8N8HfgQWBPKIPzSTDnfDnwT1X9HUBVfw1xjH4I5rwVqOV9rg2sD2F8vlDVT4HfylhlMPCSOvOAOiLSuDzHqCqJvynwU5Hptd68EtdR1RxgK5Aakuj8Ecw5FzUCd5cQ7Q563t5X3+aq+n4oA/NRMH/rI4AjROQLEZknIgNDFp1/gjnv0cAfRGQtMB24LjShhVV5/79/gKitx2+CJyJ/ADKAE8Idi99EJAZ4FBgW5lBCLRbX3NMX983uUxHppKpbwhqV/4YCL6jqIyJyLPCyiHRU1bxwBxbJqsod/zqgeZHpZt68EtcRkVjc18LNIYnOH8GcMyLSH7gTOFNV94YoNj8d7LxrAh2B2SKyGtcGOjXKH/AG87deC0xV1WxV/QH4H+5CEM2COe8RwJsAqjoXSMQVM6vKgvr/flmqSuJfABwuIq1FJB738HbqfutMBS71Pp8H/Fe9JyVR6qDnLCJdgGdxSb8qtPnCQc5bVbeqan1VbaWqrXDPNs5U1czwhFspgvnv+13c3T4iUh/X9LMqlEH6IJjzXgP0AxCRI3GJv6qPxzoVuMTr3dMT2KqqG8qzgyrR1KOqOSJyLfAfXE+A51V1uYj8DchU1anABNzXwO9wD04uDF/Ehy7Ic34YSAHe8p5jr1HVM8MWdCUI8ryrlCDP+T/AySKyAsgFblXVaP5GG+x53ww8JyKjcA96h0X5DR0i8jruIl7fe3ZxNxAHoKrjcM8yTgW+A3YBw8t9jCj/NzLGGFNOVaWpxxhjTJAs8RtjTDVjid8YY6oZS/zGGFPNWOI3xphqxhK/qZJEJE1EPheRZSJyVpH5U0SkSThj8+K4UkQu8T4Pi4SYTPVhid9UVUOBcbhCXzcCiMgZwCJVDVkhLxEJlDRfVcep6kve5DDAEr8JGUv8pqrKBmoACUCuV6bjRuCh0jYQkfO9bwhficin3rxh3reE2SKyUkTuLrL+uyKy0Kt/P7LI/B0i8oiIfAUcKyIPFBkXYYy3zmgRuUVEzsPVUXpVRBaLyGki8m6RfQ0QkXcq95/GVHf2ApepkkSkNvAa0BC4HTgK2KaqL5SxzVJgoKquE5E6qrpFRIYB9+Pq/+zClREYpqqZIlJPVX8TkSRv/gmqullEFBiiqm+KSCowB2ivqlpkv6OBHao6RkRmA7d4+xTga+B4Vd0obkCV11X1PR/+mUw1ZXf8pkryavacpqoZQBZwBjBJRJ4TkUleJcf9fQG8ICKX40oE5PtIVTer6m7gbeA4b/713l39PFzRrPyiaLnAZO/zVtyYABNE5BzcxaOsuBV4GVdquA5wLFWjnLaJIJb4TXXwV+A+XLv/57hifaP3X0lVrwT+gkviC727dXA1YIqtKiJ9gf7AsaraGTf6VaK3fI+q5nr7zME9Z5gEnA58EES8/wb+4MX7lrcPYypNlSjSZkxpRORwoJmqzhaRzri7bwWSSli3rarOB+aLyCAKS98OEJF6wG7gLOBPuIEvflfVXSLSHlf+uaTjp+CGv5wuIl9QcsXM7bhy0gCo6noRWY+7CPWv0IkbUwZL/Kaquw83HgHA67jyxXcAd5Ww7sPehUKAmbgxXtOBL3FNN82AV7y2+KXAlSLyNfAtrrmnJDWBKSKS6O33phLWeQEYJyK7cd8gdgOvAmmq+nU5z9eYg7KHu8aUwXu4m6Gq14b4uE/hup5OCOVxTfVgd/zGRBgRWQjsxNWaN6bS2R2/McZUM9arxxhjqhlL/MYYU81Y4jfGmGrGEr8xxlQzlviNMaaa+X9rTrxp3wjf9AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Weight prunning: As we increase the percentage of pruning/sparsity, the perormance will not degrade even if we prune a large portion of weights(96%), which is great!"
      ],
      "metadata": {
        "id": "16KJ3C0b2r3L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Neuron Prunning: As we increase the percentage of pruning/sparsity, the perormance will degrade, but we still can reduce the size of model to 40% and still have the 70% accuracy, which is great!"
      ],
      "metadata": {
        "id": "26n-JsKs3Ndd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Both types of pruning simplify network structure, but  nodes pruning does better job. While weights pruning can give better results at cost of  more complex ANN structure and higher   computational time. Of course  if some  data  set  have complex structure – in such cases both  approaches will end up  with   only  slightly  pruned   neural   network   – but  with   better generalization   value.\"\n",
        "\n",
        "\n",
        "Source: https://www.researchgate.net/publication/281431594_Neurons_vs_Weights_Pruning_in_Artificial_Neural_Networks"
      ],
      "metadata": {
        "id": "IUebrwv3FjmC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interesting Insights:**\n",
        "============================\n",
        "\n",
        "\n",
        "\"It is often unclear whether a network has learned a solution which will generalise to new situations or not. By deleting progressively larger and larger groups of neurons, we found that networks which generalise well were much more robust to deletions than networks which simply memorised images that were previously seen during training. In other words, networks which generalise better are harder to break (although they can definitely still be broken). \"\n",
        "\n",
        "\n",
        "\"These findings demonstrate the power of using techniques inspired by experimental neuroscience to understand neural networks. Using these methods, we found that highly selective individual neurons are no more important than non-selective neurons, and that networks which generalise well are much less reliant on individual neurons than those which simply memorise the training data. These results imply that individual neurons may be much less important than a first glance may suggest.\"\n",
        "\n",
        "Source: https://deepmind.com/blog/article/understanding-deep-learning-through-neuron-deletion\n",
        "\n",
        "\n",
        "\n",
        "\"This has been observed in several papers and can be interpreted as a post-training network regularization. Even if you’re not looking to compress your model, you might want to have a go at pruning as a way to improve its generalization performance.\"\n",
        "\n",
        "Source: https://blog.dataiku.com/making-neural-networks-smaller-for-better-deployment-solving-the-size-problem-of-cnns-using-network-pruning-with-keras\n",
        "\n",
        "================================================================"
      ],
      "metadata": {
        "id": "0rDZ2rgA5qiJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I found this article interesting about speeding up the execution of neural networks:\n",
        "\n",
        "https://ai.googleblog.com/2021/03/accelerating-neural-networks-on-mobile.html"
      ],
      "metadata": {
        "id": "aBOCZXW6BdRF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GbujcjC-Bdmv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}